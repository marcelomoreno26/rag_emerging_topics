version: "3.8"

services:
  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.4
    command: --model-id ${TEI_MODEL:-intfloat/multilingual-e5-small}
    ports:
      - "8080:80"
    volumes:
      - ./tei_data:/data
    # mem_limit: 4g

  llamacpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "8081:8080"
    volumes:
      - ./Qwen3-0.6B-Q4_K_M.gguf:/models/model.gguf
    command:
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
      - "--model"
      - "/models/model.gguf"
      - "-c"
      - "6024"
      - "--n_gpu_layers"
      - "0"

  rag:
    build: .
    ports:
      - "8000:8000"
    depends_on:
      - tei
      - llamacpp
    volumes:
      - ./index_storage:/app/index_storage
      - ./data:/app/data
